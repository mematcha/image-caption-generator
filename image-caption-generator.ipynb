{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Caption Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a to-do guide about generating captions for images and it's comparision to the latest models for generating these captions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image caption generation models are models that analyze images and automatically generate relevant captions. \n",
    "\n",
    "They combine techniques from computer vision and natural language processing to “understand” an image's visual content and express it in natural language. This task is complex because it requires not only recognizing objects in an image but also understanding their context, relationships, and the ability to translate this understanding into a coherent sentence.\n",
    "\n",
    "## Intuition\n",
    "\n",
    "- Images can be compressed to vectors of a multitude of features. These can be generated using a CNN (Convolutional Neural Network).\n",
    "\n",
    "- Our goal is to generate a suitable `caption` for the image given, which is a sequence of texts. We can generate a sequence using an RNN (Recurrent Neural Network) like LSTM(Long-Short Term Memory) or GRU (Gated Recurrent Unit)\n",
    "\n",
    "- We push the Image vector(feature vector) as our initial state for RNN and try to generate text at each time-step of the RNN using the feature vector.\n",
    "\n",
    "- While training, we will already have our images and captions at the ready. Get our feature vector of the image and push the feature vector against a untrained/ pre-trained RNN and compare it with our actual caption output. Train it with back-prop to get better at accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the pretrained `Inception_V3` model to generate the feature vector of the image.\n",
    "\n",
    "- Pass it through an RNN to generate an output embedding and compare it to the actual output in the embedding form, use an error function with these two and backprop to get a fix of this hybrid model, to generate accurate captions. \n",
    "\n",
    "- We are going to implement both `LSTM` and `GRU` architectures as our caption generation models.\n",
    "\n",
    "- We are using the `MSCOCO` Dataset for our task of image caption generation, with an 80-20 train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy tensorflow\n",
    "%pip install keras # For latest versions of tensorflow, it is advised to use keras externally \n",
    "%pip install keras_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, importlib_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_module_version(module_name):\n",
    "    try:\n",
    "        version = importlib.metadata.version(module_name)\n",
    "        print(f\"{module_name} Version: \",version)\n",
    "    except importlib.metadata.PackageNotFoundError:\n",
    "         print(f\"{module_name} is not installed or version information is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print_module_version(\"numpy\")\n",
    "import pandas as pd\n",
    "print_module_version(\"pandas\")\n",
    "import tensorflow as tf\n",
    "print_module_version(\"tensorflow\")\n",
    "from keras.applications import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, GRU, Dense, Dropout, Add\n",
    "from keras_nlp.tokenizers import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "print_module_version(\"keras\")\n",
    "# Importing Pycoctools for potential dataset handling from the coco[\"train2017\"] API -- Python Version\n",
    "import pycocotools\n",
    "print_module_version(\"pycocotools\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "print_module_version(\"sklearn\")\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "print_module_version(\"nltk\")\n",
    "from scipy.spatial.distance import cosine\n",
    "print_module_version(\"scipy\")\n",
    "\n",
    "import pickle\n",
    "print_module_version(\"pickle\")\n",
    "import os\n",
    "print_module_version(\"os\")\n",
    "import glob\n",
    "print_module_version(\"glob\")\n",
    "from PIL import Image\n",
    "print_module_version(\"PIL\")\n",
    "from tqdm import tqdm\n",
    "print_module_version(\"tqdm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "import skimage.io as io\n",
    "print_module_version(\"skimage.io\")\n",
    "import matplotlib.pyplot as plt\n",
    "print_module_version(\"matplotlib\")\n",
    "import pylab\n",
    "print_module_version(\"pylab\")\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing MSCOCO Dataset and Understanding the COCO API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have installed it with Github Import from [CocoAPI](https://github.com/cocodataset/cocoapi)\n",
    "- Used make tool to install from MakeFile of the `cocoapi/PythonAPI` folder in the repository, with the command below. \n",
    "\n",
    "$$ make -f MakeFile $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this has only provided us with the validation datasets. What we actually want are all the datasets -- train, val, test. Foe which we used the `pycocotools` module/ API for installing the COCO dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the Common Objects in Context (COCO) dataset, an annotation is a list of objects in an image, along with detailed information about each object. This information includes the object's class label, bounding box coordinates, and segmentation mask. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Annotations are stored in a JSON file, along with other information about the images and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance Viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='./dataset'\n",
    "dataTypes=['train2017','val2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coco_ds_files(datadir,datatypes):\n",
    "    coco = dict()\n",
    "    for dataType in dataTypes:\n",
    "        annFile='{}/annotations/instances_{}.json'.format(dataDir,dataType)\n",
    "        coco[dataType]=COCO(annFile)\n",
    "    return coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco = generate_coco_ds_files(datadir=dataDir,datatypes=dataTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coco['train2017'].info())\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(coco['val2017'].info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco['train2017'].getAnnIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display COCO categories and supercategories\n",
    "cats = coco[\"train2017\"].loadCats(coco[\"train2017\"].getCatIds())\n",
    "nms=[cat['name'] for cat in cats]\n",
    "print('COCO categories: \\n{}\\n'.format(' '.join(nms)))\n",
    "\n",
    "nms = set([cat['supercategory'] for cat in cats])\n",
    "print('COCO supercategories: \\n{}'.format(' '.join(nms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all images containing given categories, select one at random\n",
    "catIds = coco[\"train2017\"].getCatIds(catNms=['person','dog','skateboard']);\n",
    "print(len(catIds))\n",
    "if(len(catIds)<=5):\n",
    "    print(catIds)\n",
    "imgIds = coco[\"train2017\"].getImgIds(catIds=catIds)\n",
    "print(len(imgIds))\n",
    "if(len(imgIds)<=5):\n",
    "    print(imgIds)\n",
    "# Get a Random Image from the above categories\n",
    "img = coco[\"train2017\"].loadImgs(imgIds[np.random.randint(0,len(imgIds))])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = io.imread(img['coco_url'])\n",
    "plt.axis('off')\n",
    "plt.imshow(I)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and display instance annotations\n",
    "plt.imshow(I); plt.axis('off')\n",
    "annIds = coco[\"train2017\"].getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None)\n",
    "anns = coco[\"train2017\"].loadAnns(annIds)\n",
    "coco[\"train2017\"].showAnns(anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caption Viewing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below demostrates loading the captions of the dataset based on image id of the COCO annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='./dataset'\n",
    "dataTypes=['train2017','val2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coco_ds_caption_files(datadir,datatypes):\n",
    "    coco = dict()\n",
    "    for dataType in dataTypes:\n",
    "        annFile='{}/annotations/captions_{}.json'.format(dataDir,dataType)\n",
    "        coco[dataType]=COCO(annFile)\n",
    "    return coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_caps = generate_coco_ds_caption_files(datadir=dataDir,datatypes=dataTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and display caption annotations\n",
    "annIds = coco_caps[\"train2017\"].getAnnIds(imgIds=img['id'])\n",
    "print(annIds)\n",
    "anns = coco_caps[\"train2017\"].loadAnns(annIds)\n",
    "coco_caps[\"train2017\"].showAnns(anns)\n",
    "plt.imshow(I); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a solid understanding of what to do in order to load the COCO dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model_pretrained = InceptionV3(weights='imagenet',classifier_activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
